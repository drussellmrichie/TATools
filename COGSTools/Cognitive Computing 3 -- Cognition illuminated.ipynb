{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Computing 3 -- Cognition illuminated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my last two lectures, you saw a good portion of Python. Enough to do quite a lot, either in [your own (academic or professional) life](https://automatetheboringstuff.com/), or in cognitive science -- building programs that *do* cognitive functions.\n",
    "\n",
    "I now want to talk about some concepts and issues in cognitive science that are illuminated by (Python) programming, as well as some issues in computing that have implications for cognition.\n",
    "\n",
    "The plan for this lecture:\n",
    "1. What is a 'computational model' of cognition, and why do we build them?\n",
    "2. Marr's levels of analysis of cognitive/computing systems\n",
    "3. Modularity (and preview of Wednesday's lecture on Modularity of Mind)\n",
    "4. A *very* informal, nontechnical introduction to 'algorithmic complexity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** To illustrate these points, let's consider the following problem:**\n",
    "\n",
    "When I'm listening to someone speak, how do I decipher the sounds of the words they utter? For example, how do I determine if someone said 'bad' or 'bat'? What distinguishes 'd' from 't'?\n",
    "\n",
    "There's a lot of physical characteristics of the sound hitting my ear that differentiates 'd' from 't', but one is the *length of the vowel before these two consonants*. The 'a' before 'd' is longer than the 'a' before 't'. You can all probably just demonstrate this to yourselves now by comparing your pronunciations of 'bad' and 'bat'.\n",
    "\n",
    "And in fact, humans do use such cues to categorize speech sounds. So let's imagine the following very simple...\n",
    "\n",
    "## Computational model of speech sound categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "def d_or_t(preceding_vowel_length):\n",
    "    # imagine that vowel length is in milliseconds\n",
    "    if preceding_vowel_length > 200: #i'm just guessing that this is a reasonable boundary!\n",
    "        return 'd'\n",
    "    else:\n",
    "        return 't'\n",
    "\n",
    "print( d_or_t( preceding_vowel_length=250 ) )\n",
    "print( d_or_t( preceding_vowel_length=150 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So that is a computational model of cognition because it is a computationally-explicit theory of the process by which we accomplish some cognitive function. Put another way, it specifies an algorithm by which we *do* this particular function.**\n",
    "\n",
    "Awesome! We solved speech recognition!\n",
    "\n",
    "Well, not really. Let's also consider the following: people speaker at different speeds, and correspondingly, people vary in their typical boundaries between consonants like 'd' and 't'.\n",
    "\n",
    "Okay, let's try to account for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "def d_or_t_extended(preceding_vowel_length, speaker):\n",
    "    \n",
    "    if speaker == 'Russell': # imagining if I was an atypically fast speaker\n",
    "        boundary = 150\n",
    "    else:\n",
    "        boundary = 200\n",
    "\n",
    "    if preceding_vowel_length > boundary: \n",
    "        return 'd'\n",
    "    else:\n",
    "        return 't'\n",
    "        \n",
    "print( d_or_t_extended( preceding_vowel_length=175, speaker='Russell' ) )\n",
    "print( d_or_t_extended( preceding_vowel_length=175, speaker='Garrett' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we make such computational models of cognition?**\n",
    "\n",
    "These models were pretty simple and we could intuit their behavior pretty easily. But it doesn't take much extra complexity before the models' behaviors become less predictable. At that point, it helps to *run* an implemented model and *see* what behaviors follow from the model. Sometimes you'll find that the behavior of the model isn't what you expected, and/or that it doesn't produce the phenomenon that it was supposed to! And sometimes in the process of *programming* the model, you'll be forced to make explicit some aspect of a theory that was only vague before. Maybe at that point you realize that the theory doesn't actually make sense!\n",
    "\n",
    "**Questions??**\n",
    "\n",
    "We can now also use these two different versions of speech categorization to discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marr's levels of analysis\n",
    "\n",
    "(Apologies for this section being text-heavy)\n",
    "\n",
    "If you read the assigned Bermudez, some of this exposition is going to be a bit of review (which is a good thing!).\n",
    "\n",
    "David Marr was a vision scientist concerned with specifying *what* computations the visual system carries out, *how* it does so, and *what* the physical basis of the system is.\n",
    "\n",
    "1. **Computational level**: *what* computation or function is being performed? What is its goal(s) What information does the system start with (inputs), and what information must it produce (outputs)?\n",
    "2. **Representation and algorithm level**: *how* does the system do what it does? Specifically, what representations does it use and what processes does it employ to build and manipulate the representations? *What is the algorithm by which the computation/function from level 1 is accomplished?*\n",
    "3. **Hardware implementation level**: *how* is the system physically realised? A biological brain composed of networks of neurons? A silicon circuitboard (as in a laptop)? *How do the neurons or silicon circuits implement the algorithm(s) from level 2?*\n",
    "\n",
    "Marr claimed that (one of) the task(s) of the visual system was to derive a 3d representation and orientation of an object, such that the object could later be recognized (as a dog, cat, whatever). This is basically a **computational level** theory of the visual system, a claim about *what* function the system performs.\n",
    "\n",
    "I won't go into Marr's algorithmic and implementational level theories of that task. Instead, let's shift our attention back to our `d_or_t` functions and illustrate these levels there.\n",
    "\n",
    "Both our implemented functions have similar computational level analyses: their task/goal is to determine if the sound was a 'd' or a 't'. The only respect in which they differ at the computational level is the information/inputs they are provided -- our second function has speaker information, while the first does not.\n",
    "\n",
    "Where they differ more substantially is at the algorithmic level. `d_or_t` simply checks if the preceding vowel length is above or below a single threshold. But the algorithm of `d_or_t_extended` also changes the threshold depending on who is speaking.\n",
    "\n",
    "For the time being, it's hard for us to get much more specific about the implementational level, and that's pretty true of cognitive science in general. We might have fairly elaborate theories of how, for example, we parse an English sentence into a syntactic tree, but we don't have as detailed theories about how a brain does that.\n",
    "\n",
    "**Questions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity\n",
    "\n",
    "In the next class, I'm going to lecture on a fundamental theory in cognitive science called Modularity of Mind. Its basic claim is that (portions of) minds are made up of different parts -- called modules -- that have different functions/purposes, and these parts communicate with each other to only a limited extent.\n",
    "\n",
    "Programming, and functions in particular, will insantiate this concept of modularity nicely -- Python/programming functions are basically modules. Let's think about our 'd' or 't' categorization problem again. To remind you all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d_or_t(preceding_vowel_length):\n",
    "    # imagine that vowel length is in milliseconds\n",
    "    if preceding_vowel_length > 200: #i'm just guessing that this is a reasonable boundary!\n",
    "        return 'd'\n",
    "    else:\n",
    "        return 't'\n",
    "    \n",
    "def d_or_t_extended(preceding_vowel_length, speaker):\n",
    "    if speaker == 'Russell': # imagining if I was an atypically fast speaker\n",
    "        boundary = 150\n",
    "    else:\n",
    "        boundary = 200\n",
    "\n",
    "    if preceding_vowel_length > boundary: \n",
    "        return 'd'\n",
    "    else:\n",
    "        return 't'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the d/t sound comes after 'rea?' -- the '?' is representing the d/t sound that needs to be categorized. As far as I know, 'read' is a word, but 'reat' is not. So, a categorization function could theoretically rely on that information and be less likely to categorize a sound as 't' after 'rea'.\n",
    "\n",
    "**But both of our current `d_or_t` models are not sensitive to that information. We could say that they are modularized with respect to word context of the d/t sound.**\n",
    "\n",
    "So of course, the addition of the variable `preceding` does not change the operation of `d_or_t` in this next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reat\n"
     ]
    }
   ],
   "source": [
    "preceding = 'rea'\n",
    "whole_word = preceding + d_or_t(150)\n",
    "print(whole_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows how the function `d_or_t` is *encapsulated* from certain information outside the function. Modularity also often includes the reverse relation: that the function doesn't expose all of *its* information to the outside. In modularity of mind, we'll learn that that is called *cognitive impenetrability*.\n",
    "\n",
    "Our extended function illustrates this. Inside the extended function, a variable called `boundary` is defined. However, that information *only exists inside the function*. If we try to access that variable outside the function, we fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boundary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-90ecb57b2d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md_or_t_extended\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpreceding_vowel_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m175\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Russell'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboundary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'boundary' is not defined"
     ]
    }
   ],
   "source": [
    "d_or_t_extended( preceding_vowel_length=175, speaker='Russell' )\n",
    "print(boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to recap, Python functions and the contexts in which they are called are *modularized with respect to one another*, at least to varying degrees.\n",
    "\n",
    "We'll develop this theory further on Wednesday. **And we'll see that it's actually a huge debate, raging to this day, the extent to which functions like speech sound categorization are modularized, i.e., sensitive to this or that kind of information.**\n",
    "\n",
    "**Questions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic complexity\n",
    "\n",
    "Let's talk briefly a bit about properties of the second of Marr's levels. That is, properties of particular algorithms. Properties like how much time and memory an algorithm requires.\n",
    "\n",
    "If you were to take CSE classes, you'd study different algorithms for functions like sorting (e.g., sequences of numbers) or parsing. For example, if you are sorting `n` items, some algorithms take `n * log(n)` time, on average, to finish, whereas others take `n**2` time (on average).\n",
    "\n",
    "For this class, we won't get nearly so technical. I just want to illustrate in a general way how the resource requirements of a computational model can depend on its input, and have you all start thinking about what those requirements mean for cognition. In particular, if an algorithm is overly greedy of time and memory, it probably isn't a realistic model of human cognition.\n",
    "\n",
    "To loosely illustrate this idea of algorithmic complexity, let's try to model that last idea about speech categorization, where categorization depends on whether a particular decision of 'd' or 't' makes a real English word or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d_or_t_final(preceding_vowel_length, preceding_context, english_words):\n",
    "    # two new variables: preceding_context and english_words\n",
    "    \n",
    "    t_completion = preceding_context + 't' # word made by a 't' decision\n",
    "    d_completion = preceding_context + 'd' # word made by a 'd' decision\n",
    "    \n",
    "    # each of these new variables will be true if the completion is in english_words\n",
    "    # and False otherwise\n",
    "    d_plausible = d_completion in english_words\n",
    "    t_plausible = t_completion in english_words\n",
    "    \n",
    "    # equivalent to `if (t_possible == True) and (d_possible == False)`\n",
    "    # in English, if the t_completion is plausible but the d_completion is not...or vice versa\n",
    "    if t_plausible and not d_plausible:\n",
    "        return 't'\n",
    "    if d_plausible and not t_plausible:\n",
    "        return 'd'\n",
    "\n",
    "    # if the function has gotten to this point, it means that the t_completion and d_completion\n",
    "    # are equally plausible, so now the function should just rely on the preceding vowel length\n",
    "    if preceding_vowel_length > 200: \n",
    "        return 'd'\n",
    "    else:\n",
    "        return 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = ['read','bead','beat','cat'] # ignore the fact that cad is a real if rare word...\n",
    "d_or_t_final(100, preceding_context='rea',english_words = my_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can someone walk me through how, exactly, `d_or_t_final` produced that output? What are the variables' values at each line of the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out some other possible inputs. We'll use the `assert` statement. (This will come up on the PS so I'm showing it now.) It basically requires means \"This expression better be true or I'm raising an error!\".\n",
    "\n",
    "So the next line works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 1 != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this line fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-34d369bab8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try three classes of test cases:\n",
    "1. In the first two cases, one word completion is sensible (e.g., 'read') while the other is not (e.g., 'reat'), so the function chooses 't' or 'd' on that basis, even if the vowel length favors the other consonant!\n",
    "2. In the next two cases 'beat' and 'bead' are both equally good, so the function relies on `preceding_vowel_length`.\n",
    "3. In the final two cases, neither completion is in `my_words`, so the function relies on `preceding_vowel_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert d_or_t_final(150, preceding_context='rea',english_words = my_words) == 'd'\n",
    "assert d_or_t_final(250, preceding_context='ca', english_words = my_words) == 't'\n",
    "\n",
    "assert d_or_t_final(150, preceding_context='bea',english_words = my_words) == 't'\n",
    "assert d_or_t_final(250, preceding_context='bea',english_words = my_words) == 'd'\n",
    "\n",
    "assert d_or_t_final(150, preceding_context='scra',english_words = my_words) == 't'\n",
    "assert d_or_t_final(250, preceding_context='scra',english_words = my_words) == 'd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our function is behaving properly, the above cell shouldn't raise any errors.\n",
    "\n",
    "**Let's now think of the time complexity of `d_or_t_final`.** Does anyone have any guesses about which input to the function could get bigger and bigger and bigger, and hence slow down the model? (As a reminder, the inputs are `preceding_vowel_length`, `preceding_context`, and `english_words`.) Why would size of that particular input influence the speed of the model? (This very well might be hard to guess!)\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "The size of `english_words` will influence how long the model takes to run. This is because in the first two `if` statements, the program has to check the words in `english_words` one by one and check if the there's a match to the two possible completions in it. The longer the list is, the longer the search takes.\n",
    "\n",
    "This can be deduced from reasoning about the code, but we can also simply show it with testing. We'll just record how long it takes the function to run with `english_words` of two different sizes!\n",
    "\n",
    "(Don't worry about some of the new functionality in this next cell like `import time`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001270771026611328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "my_words = ['read','bead','beat','cat']\n",
    "categorization = d_or_t_final(250, preceding_context='scra',english_words = my_words)\n",
    "duration   = time.time() - start_time\n",
    "\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if `my_words` was a very long list.....again, don't worry about how exactly I'm reading in this file of words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a-horizon', 'a-ok', 'aardvark', 'aardwolf', 'ab', 'aba', 'abaca', 'abacist', 'aback']\n",
      "69905\n"
     ]
    }
   ],
   "source": [
    "with open('wordlist.txt') as f:\n",
    "    my_words = [x.strip() for x in f.readlines()]\n",
    "print(my_words[:10])\n",
    "print(len(my_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 70,000 words now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003420114517211914\n",
      "26.913696060037523\n"
     ]
    }
   ],
   "source": [
    "start_time     = time.time()\n",
    "categorization = d_or_t_final(250, preceding_context='scra', english_words=my_words)\n",
    "new_duration   = time.time() - start_time\n",
    "\n",
    "print(new_duration)\n",
    "print(new_duration/duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took *way* longer -- ~26 times longer!\n",
    "\n",
    "The homework asks you to reason about a similar computational model of cognition. As with this case, you can reason about the model by analyzing its code, or you can test it empirically.\n",
    "\n",
    "**Questions??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "The homework will ask you to analyze similar, fairly simple cognitive models in terms of Marr's Levels, modularity, run-time, and more: it'll ask you to muse on what's unrealistic about a model, and it'll ask you to finish programming a model.\n",
    "\n",
    "I urge everyone to start trying to work with the problem set in the jupyter notebook, and probably also try the optional exercises. ***Please come to office hours before it's too late!***\n",
    "\n",
    "**Questions???**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
