{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 2201: Problem set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cohort Model of Word Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem set, you will first work with an implementation of the [Cohort Model of word recognition](http://psycholinguistics.wikifoundry.com/page/The+Cohort+Model+of+Word+Recognition), devised by William Marslen-Wilson. Read that link, but in brief, it is a model of how people understand a spoken word as it unfolds over time. For example, as the word 'cat' is heard bit by bit -- 'c', 'ca', 'cat' -- how are we activating words (and hopefully eventually the right word!) in our head?\n",
    "\n",
    "Below is a quick and dirty implementation of the Cohort Model that I wrote. Read through it, and see if you can understand what it's doing, and how it implements the description of the model in the link above. (It may be easier once you see an example output of the model a couple cells below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cohortModel(word, EnglishWords):\n",
    "    soFar = ''                                # when we start listening to a word, we of course haven't heard anything yet, so we'll represent that as an empty string\n",
    "    activated = list(EnglishWords)            # we'll maintain a list of the activated words...we'll start by assuming all the words we know are possible and thus activated\n",
    "    timeCourse = [ list(activated) ]          # we'll make a list -- timeCourse -- of the activated words after each letter; so this will be a list of lists of strings!\n",
    "    for letter in word:                       # then start listening to the word letter by letter\n",
    "        soFar = soFar + letter                # add the newly heard letter to the portion of the word heard so far\n",
    "        for word in list(activated):          # now look through the currently activated words\n",
    "            if not word.startswith(soFar):    # if this particular currently activated word is inconsistent with the input so far\n",
    "                activated.remove(word)        # ...then deactivate that word\n",
    "        timeCourse.append( list(activated) )  # add currently activated words to timeCourse...and continue the loop  \n",
    "    return timeCourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run the cell above, test the cohort model with a fragment of English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EnglishWords = ['cathedral', 'cat','dog','catheter']\n",
    "context      = \"I went to London and saw St. Paul's \"\n",
    "word         = 'cathedral'\n",
    "testRun      = cohortModel(word, EnglishWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show what the `timeCourse` of the word recognition process is for our `testRun`. It is  less critical to understand how this next chunk works. It is just printing the output (and part of the input) in an interpretable fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After letter _, the activated words are ['cathedral', 'cat', 'dog', 'catheter']\n",
      "After letter c, the activated words are ['cathedral', 'cat', 'catheter']\n",
      "After letter a, the activated words are ['cathedral', 'cat', 'catheter']\n",
      "After letter t, the activated words are ['cathedral', 'cat', 'catheter']\n",
      "After letter h, the activated words are ['cathedral', 'catheter']\n",
      "After letter e, the activated words are ['cathedral', 'catheter']\n",
      "After letter d, the activated words are ['cathedral']\n",
      "After letter r, the activated words are ['cathedral']\n",
      "After letter a, the activated words are ['cathedral']\n",
      "After letter l, the activated words are ['cathedral']\n"
     ]
    }
   ],
   "source": [
    "for letter, timeSlice in zip('_' + word, testRun):\n",
    "    print(\"After letter {}, the activated words are\".format(letter),timeSlice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer some questions about the model!\n",
    "\n",
    "** Question 1:** Describe the *above implementation* of the cohort model in terms of Marr's three levels. One or two sentences per level will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** How does the run-time of the model depend on the size of the lexicon, `EnglishWords`, that it is given? In other words, how does the length of time the computer takes to run `cohortModel` depend on the size of `EnglishWords`? You can answer this in an informal way -- no need to get mathematically precise. You can answer this just by analyzing the code, but you could also test your analysis by running the model with lexicons of different sizes. If you pursue this path, you may find what I wrote below helpful..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0554327964782715\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def squares_to_n(n):\n",
    "    squares = []\n",
    "    for x in range(n):\n",
    "        squares.append(x)\n",
    "    return squares\n",
    "\n",
    "start_time = time.time()\n",
    "my_squares = squares_to_n(10**7)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** If my implementation of the cohort model were an accurate model of human word recognition (it's not, but pretend it is!), then what empirical predictions does your answer to question two make of human performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Discuss the modularity of this model. What information passes between the model and the broader program? What information does not? Put another way, what information does model expose, or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A silly but hopefully instructive model of categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went through all that trouble of teaching you some Python basics. We should probably have you write something, too, huh? How about this:\n",
    "\n",
    "Later in the semester we'll talk about concepts and categorization. That is, what exactly are concepts like 'water' or 'freedom', and when we come across things in the world, how do we decide what concept or category a thing belongs to?\n",
    "\n",
    "Let's preview this a little bit, by imagining an incredibly simple world and model of categorization: to decide if something is a cat or a dog (supposing, for this moment, that no other concepts exist), you simply count up whether it has more cat-like features or more dog-like features. That is, if it meows, is small, and plays with string, it's a cat. If it's big, barks, and plays with bones, it's a dog. If it's some mix of those, then it's simply whatever animal it shares more features with.\n",
    "\n",
    "**Queston 5:** Write a `dog_or_cat` function that takes a list of features for a given object and determines whether that object is a cat or a dog.\n",
    "\n",
    "I've gotten you started a bit below, and here's a hint to get you going: maybe you want to check each individual feature of the object and see whether it's in `cat_features` or `dog_features`? So you'll definitely be needing to write a loop and conditional (if/then) or two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dog_or_cat(object_features):\n",
    "    \"\"\"\n",
    "    object_features: list of features like 'meows', 'barks', 'small', 'big', etc.\n",
    "    \"\"\"\n",
    "    cat_features = ['meows','small','plays-with-string']\n",
    "    dog_features = ['barks','big',  'plays-with-bone']\n",
    "    \n",
    "    similarity_to_cat = 0\n",
    "    similarity_to_dog = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Insert your code below\n",
    "    \"\"\"\n",
    "    \n",
    "    # maybe given them everthing except the loop + embedded if/else, \n",
    "    # as well as the second if/else?\n",
    "    \n",
    "    for feature in object_features:\n",
    "        if feature in cat_features:\n",
    "            similarity_to_cat = similarity_to_cat + 1\n",
    "        else:\n",
    "            similarity_to_dog = similarity_to_dog + 1\n",
    "\n",
    "    if similarity_to_cat > similarity_to_dog:\n",
    "        category = 'cat'\n",
    "    else:\n",
    "        category = 'dog'\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model. Of course, if you programmed it right, this next cell should output `'cat'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_object = ['meows','small','plays-with-string']\n",
    "dog_or_cat(test_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try more test-cases. Again, if your function was written appropriately, then running the next chunk should not generate any errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert dog_or_cat(['meows','small','plays-with-string']) == 'cat'\n",
    "assert dog_or_cat(['barks','big',  'plays-with-bone'])   == 'dog'\n",
    "assert dog_or_cat(['meows','small','plays-with-bone'])   == 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** What about this model of categorization seems inaccurate or incomplete with respect to concepts and categorization? How might you modify this model to rectify these issues? You can just discuss this verbally (in a few sentences), but do feel free to try implementing some modifications with Python, if you are so inclined! (If you chose this latter route, you could copy and paste the `dog_or_cat` definition from above into a new cell below, and then modify, rename, and test it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
